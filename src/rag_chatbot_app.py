import streamlit as st
import tempfile
import os
import time
import torch
import asyncio

from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_huggingface.llms import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from transformers import BitsAndBytesConfig
from utils import process_multiple_pdfs
from models import load_llm as _load_llm


MODEL_LIST = ["lmsys/vicuna-7b-v1.5", "vinai/PhoGPT-4B-Chat"]

# Session state initialization
if 'model_name' not in st.session_state:  # Error syntax: if not st.session_state.model_name
        st.session_state.model_name = MODEL_LIST[0] # Set the first model as default
if 'rag_chain' not in st.session_state:
    st.session_state.rag_chain = None
if 'models_loaded' not in st.session_state:
    st.session_state.models_loaded = False
if 'embeddings' not in st.session_state:
    st.session_state.embeddings = None
if 'llm' not in st.session_state:
    st.session_state.llm = None
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'pdf_processed' not in st.session_state:
    st.session_state.pdf_processed = False
if 'pdf_names' not in st.session_state:  # Changed to store multiple PDF names
    st.session_state.pdf_names = []
if 'total_chunks' not in st.session_state:  # Store total chunks from all PDFs
    st.session_state.total_chunks = 0
if 'lang' not in st.session_state:
    st.session_state.lang = "vi"

#Add language feature
LANG_OPTIONS = {
    "vi": "Tiáº¿ng Viá»‡t",
    "en": "English"
}

translations = {
    "title": {
        "vi": "ğŸ“„ Trá»£ lÃ½ PDF RAG",
        "en": "ğŸ“„ PDF RAG Assistant"
    },
    "description": {
        "vi": "*TrÃ² chuyá»‡n vá»›i Chatbot Ä‘á»ƒ trao Ä‘á»•i vá» ná»™i dung tÃ i liá»‡u PDF cá»§a báº¡n*",
        "en": "*Chat with the chatbot to explore your PDF content*"
    },
    "model_select": {
        "vi": "Chá»n model AI",
        "en": "Select AI model"
    },
    "upload_label": {
        "vi": "Chá»n file PDF",
        "en": "Upload PDF file"
    },
    "process_button": {
        "vi": "ğŸ”„ Xá»­ lÃ½ PDF",
        "en": "ğŸ”„ Process PDF"
    },
    "pdf_ready": {
        "vi": "ğŸ“„ ÄÃ£ táº£i",
        "en": "ğŸ“„ Uploaded"
    },
    "pdf_empty": {
        "vi": "ğŸ“„ ChÆ°a cÃ³ tÃ i liá»‡u",
        "en": "ğŸ“„ No PDF uploaded"
    },
    "chat_input_placeholder": {
        "vi": "Nháº­p cÃ¢u há»i cá»§a báº¡n...",
        "en": "Enter your question..."
    },
    "chat_disabled": {
        "vi": "ğŸ”„ Vui lÃ²ng upload vÃ  xá»­ lÃ½ file PDF trÆ°á»›c khi báº¯t Ä‘áº§u chat!",
        "en": "ğŸ”„ Please upload and process a PDF before chatting!"
    },
    "thinking": {
        "vi": "Äang suy nghÄ©...",
        "en": "Thinking..."
    },
    "model_loading": {
        "vi": "â³ Äang táº£i AI models, vui lÃ²ng Ä‘á»£i...",
        "en": "â³ Loading AI models, please wait..."
    },
    "model_ready": {
        "vi": "âœ… Models Ä‘Ã£ sáºµn sÃ ng!",
        "en": "âœ… Models are ready!"
    },
    "clear_chat": {
        "vi": "ğŸ—‘ï¸ XÃ³a lá»‹ch sá»­ chat",
        "en": "ğŸ—‘ï¸ Clear chat history"
    },
    "instructions": {
        "vi": "**CÃ¡ch sá»­ dá»¥ng:**\n1. **Chá»n model**\n2. **Upload PDF**\n3. **Äáº·t cÃ¢u há»i**\n4. **Nháº­n tráº£ lá»i**",
        "en": "**How to use:**\n1. **Select model**\n2. **Upload PDF**\n3. **Ask questions**\n4. **Receive answers**"
    },
    "used_model":
    {
        "vi": "Model AI Ä‘Æ°á»£c sá»­ dá»¥ng: ",
        "en": "Model AI being used: "
    },
    "setting":
    {
        "vi": "âš™ï¸ CÃ i Ä‘áº·t",
        "en": "âš™ï¸ Settings"
    },
    "upload_pdf":
    {
        "vi": "Upload TÃ i liá»‡u",
        "en": "Upload PDF"
    }
}

# Functions
@st.cache_resource
def load_embeddings():
    return HuggingFaceEmbeddings(model_name="bkai-foundation-models/vietnamese-bi-encoder")

load_llm = st.cache_resource(_load_llm)

def add_message(role, content):
    """ThÃªm tin nháº¯n vÃ o lá»‹ch sá»­ chat"""
    st.session_state.chat_history.append({
        "role": role,
        "content": content,
        "timestamp": time.time()
    })

def clear_chat():
    """XÃ³a lá»‹ch sá»­ chat"""
    st.session_state.chat_history = []

def display_chat():
    """Hiá»ƒn thá»‹ lá»‹ch sá»­ chat"""
    if st.session_state.chat_history:
        for message in st.session_state.chat_history:
            if message["role"] == "user":
                with st.chat_message("user"):
                    st.write(message["content"])
            else:
                with st.chat_message("assistant"):
                    st.write(message["content"])
    else:
        with st.chat_message("assistant"):
            st.write("Xin chÃ o! TÃ´i lÃ  AI assistant. HÃ£y upload file PDF vÃ  báº¯t Ä‘áº§u Ä‘áº·t cÃ¢u há»i vá» ná»™i dung tÃ i liá»‡u nhÃ©! ğŸ˜Š")

# UI
def main():
    # Sidebar: Language selection
    with st.sidebar:
        st.title("ğŸŒ Language / NgÃ´n ngá»¯")
        lang_choice = st.selectbox("Chá»n / Select", options=list(LANG_OPTIONS.keys()), format_func=lambda x: LANG_OPTIONS[x])
        st.session_state.lang = lang_choice
        t = lambda key: translations[key][st.session_state.lang]

    st.set_page_config(
        page_title="PDF RAG Chatbot", 
        layout="wide",
        initial_sidebar_state="expanded"
    )
    st.title(t("title"))
    st.logo("./logo.png", size="large")
    

    # Main content
    st.markdown(t("description"))
    
    # Chat container
    chat_container = st.container()
    
    with chat_container:
        # Display chat history
        display_chat()
    
    # Model selection
    model_name = st.selectbox(t("model_select"), MODEL_LIST, index=MODEL_LIST.index(st.session_state.model_name))
    
    # Chat input
    if st.session_state.models_loaded:
        st.info(t("used_model") + st.session_state.model_name)
        if st.session_state.pdf_processed:
            # User input
            user_input = st.chat_input(t("chat_input_placeholder"))
            
            if user_input:
                # Add user message
                add_message("user", user_input)
                
                # Display user message immediately
                with st.chat_message("user"):
                    st.write(user_input)
                
                # Generate response
                with st.chat_message("assistant"):
                    with st.spinner(t("thinking")):
                        try:
                            output = st.session_state.rag_chain.invoke(user_input)
                            # Clean up the response
                            if 'Answer:' in output:
                                answer = output.split('Answer:')[1].strip()
                            else:
                                answer = output.strip()
                            
                            # Display response
                            st.write(answer)
                            
                            # Add assistant message to history
                            add_message("assistant", answer)
                            
                        except Exception as e:
                            error_msg = f"âŒ {str(e)}"
                            st.error(error_msg)
                            add_message("assistant", error_msg)
        else:
            st.info(t("chat_disabled"))
            st.chat_input("Nháº­p cÃ¢u há»i cá»§a báº¡n...", disabled=True)
    else:
        st.info(t("model_loading"))
        st.chat_input(t("chat_input_placeholder"), disabled=True)
        
    # Sidebar
    with st.sidebar:
        st.title(t("settings"))
        
        # Load models
        if model_name != st.session_state.model_name:
            st.session_state.model_name = model_name
            st.session_state.models_loaded = False  # Force reload if needed
            st.rerun()
            
        if not st.session_state.models_loaded:
            st.warning(t("model_loading"))
            with st.spinner(t("model_loading")):
                st.session_state.embeddings = load_embeddings()
                st.session_state.llm = load_llm(st.session_state.model_name) 
                st.session_state.models_loaded = True
            st.success(t("model_ready"))
            st.rerun()
        else:
            st.success(t("model_ready"))

        st.markdown("---")
        
        # Upload PDF
        st.subheader("ğŸ“„ Upload tÃ i liá»‡u")
        uploaded_files = st.file_uploader(t("upload_label"), accept_multiple_files=True, type="pdf")
        
        if uploaded_files:
            if st.button(t("process_button"), use_container_width=True):
                with st.spinner(t("process_button")):
                    progress_bar = st.progress(0)
                    def update_progress(value):
                        progress_bar.progress(value)
                    st.session_state.rag_chain, chunk_counts = process_multiple_pdfs(uploaded_files, update_progress)
                    st.session_state.pdf_processed = True
                    st.session_state.pdf_names = [file.name for file in uploaded_files]
                    st.session_state.total_chunks = sum(chunk_counts)
                    # Reset chat history khi upload PDF má»›i
                    clear_chat()
                    add_message("assistant", f"{t('pdf_ready')}: {', '.join(st.session_state.pdf_names)}\nChunks: {st.session_state.total_chunks}")
                st.rerun()
        
        # PDF status
        if st.session_state.pdf_processed:
            st.success(f"{t('pdf_ready')}: {', '.join(st.session_state.pdf_names)}")
        else:
            st.info(t("pdf_empty"))
            
        st.markdown("---")
        
        # Chat controls
        st.subheader("ğŸ’¬ Chat")
        if st.button(t("clear_chat"), use_container_width=True):
            clear_chat()
            st.rerun()
            
        st.markdown("---")
        
        # Instructions
        st.markdown("---")
        st.subheader("ğŸ“‹ Guide")
        st.markdown(t("instructions"))

if __name__ == "__main__":
    main()
